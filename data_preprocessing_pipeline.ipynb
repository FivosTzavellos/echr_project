{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/ftzavellos/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/ftzavellos/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /Users/ftzavellos/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting en-core-web-sm==3.8.0\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.8.0/en_core_web_sm-3.8.0-py3-none-any.whl (12.8 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.8/12.8 MB\u001b[0m \u001b[31m29.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25h\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('en_core_web_sm')\n",
      "\u001b[38;5;3m⚠ Restart to reload dependencies\u001b[0m\n",
      "If you are in a Jupyter or Colab notebook, you may need to restart Python in\n",
      "order to load all the package's dependencies. You can do this by selecting the\n",
      "'Restart kernel' or 'Restart runtime' option.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import csv\n",
    "import string\n",
    "import math\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "\n",
    "import spacy\n",
    "spacy.cli.download(\"en_core_web_sm\")\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "nlp.max_length = 1500000 \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read the full dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dataset = pd.read_csv('dataset.csv', index_col=0, encoding = 'utf-8')\n",
    "df_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean up the facts and full_text columns "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    # Remove markdown elements and special characters\n",
    "    text = re.sub(r'#', '', text)  # Remove '###'\n",
    "    text = re.sub(r'[-]', '', text)  # Remove '-'\n",
    "    text = re.sub(r'\\s+', ' ', text)  # Replace multiple spaces with a single space\n",
    "    text = re.sub(r'\\\\[a-zA-Z0-9]+', '', text)  # Remove any escaped sequences like \\xa0\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dataset['facts'] = df_dataset['facts'].apply(clean_text)\n",
    "df_dataset['full_text'] = df_dataset['full_text'].apply(clean_text)\n",
    "df_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract named entities from facts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DO NOT run unless you want to redo the process, otherwise reading the named_entities.csv would suffice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "facts = df_dataset['facts']\n",
    "facts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "facts_list = facts.tolist()\n",
    "facts_list = [str(fact) for fact in facts_list]\n",
    "facts_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract named entities from facts_list\n",
    "entities = [ent.text for doc in map(nlp, facts_list) for ent in doc.ents]\n",
    "\n",
    "# Get the unique values from the entities list\n",
    "entities_list = list(dict.fromkeys(entities))\n",
    "\n",
    "# Save the list of entities\n",
    "with open('named_entities_facts.csv', 'w', newline='', encoding='utf-8') as csvfile:\n",
    "    writer = csv.writer(csvfile)\n",
    "    for ent in entities_list:\n",
    "        writer.writerow([ent])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read named_entities_facts.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pd' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m entities_facts  \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241m.\u001b[39mread_csv(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnamed_entities_facts.csv\u001b[39m\u001b[38;5;124m'\u001b[39m, header \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m, names \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfacts\u001b[39m\u001b[38;5;124m'\u001b[39m], encoding \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      2\u001b[0m entities_list_facts \u001b[38;5;241m=\u001b[39m entities_facts[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfacts\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mtolist()\n",
      "\u001b[0;31mNameError\u001b[0m: name 'pd' is not defined"
     ]
    }
   ],
   "source": [
    "entities_facts  = pd.read_csv('named_entities_facts.csv', header = None, names = ['facts'], encoding = 'utf-8')\n",
    "entities_list_facts = entities_facts['facts'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>facts</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>57064</th>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      facts\n",
       "57064   NaN"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nan_values_f = entities_facts['facts'].isna()\n",
    "entities_facts[nan_values_f]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "entities_list_facts = [str(element) for element in entities_list_facts]\n",
    "entities_list_facts = [x for x in entities_list_facts if x.lower() != \"nan\"]\n",
    "print(sum(1 for x in entities_list_facts if x.lower() == \"nan\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remove named entities from facts (creating new columns in the df_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_named_entities(text, entities_list):\n",
    "    for entity in entities_list:\n",
    "        # Use word boundaries to ensure whole phrases are matched\n",
    "        entity_regex = re.compile(r'\\b' + re.escape(entity) + r'\\b', re.IGNORECASE)\n",
    "        text = entity_regex.sub('', text)\n",
    "    return text.strip()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dataset['facts_ne_removed'] = df_dataset['facts'].apply(lambda x: remove_named_entities(x, entities_list_facts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dataset['bertsum_ne_removed'] = df_dataset['bert_summary'].apply(lambda x: remove_named_entities(x, entities_list_facts))\n",
    "df_dataset['textrank_ne_removed'] = df_dataset['textrank_summary'].apply(lambda x: remove_named_entities(x, entities_list_facts))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_stopwords = stopwords.words('english')\n",
    "all_stopwords.extend([\n",
    "'also',\n",
    "'may', \n",
    "'could', \n",
    "'would', \n",
    "'must', \n",
    "'applicant', \n",
    "'applicants'\n",
    "'court',\n",
    "'article',\n",
    "'case',\n",
    "'convetion',\n",
    "'see',\n",
    "'right',\n",
    "'government',\n",
    "'paragraph',\n",
    "'law',\n",
    "'state',\n",
    "'detention',\n",
    "'authority',\n",
    "'application',\n",
    "'one'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stopwords(text, stopwords_list):\n",
    "    for entity in stopwords_list:\n",
    "        # Use word boundaries to ensure whole phrases are matched\n",
    "        stopword_regex = re.compile(r'\\b' + re.escape(entity) + r'\\b', re.IGNORECASE)\n",
    "        text = stopword_regex.sub('', text)\n",
    "    return text.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove stopwords from named_entities_removed columns\n",
    "df_dataset['cleaned_facts'] = df_dataset['facts_ne_removed'].apply(lambda x: remove_stopwords(x, all_stopwords))\n",
    "df_dataset['cleaned_bertsum'] = df_dataset['bertsum_ne_removed'].apply(lambda x: remove_stopwords(x, all_stopwords))\n",
    "df_dataset['cleaned_textrank'] = df_dataset['textrank_ne_removed'].apply(lambda x: remove_stopwords(x, all_stopwords))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remove Verbs and Adjectives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_tags(text, pos_tags):\n",
    "    \n",
    "    words = nltk.word_tokenize(text)\n",
    "    tagged_words = nltk.pos_tag(words)\n",
    "\n",
    "    filtered_words = [word for word, tag in tagged_words if not any(tag.startswith(pos_tag) for pos_tag in pos_tags)]\n",
    "    return ' '.join(filtered_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_pos(text, remove_verbs=False, remove_adjectives=False):\n",
    "    \n",
    "    pos_tags_to_remove = []\n",
    "    if remove_verbs:\n",
    "        pos_tags_to_remove.append('VB')\n",
    "    if remove_adjectives:\n",
    "        pos_tags_to_remove.append('JJ')\n",
    "\n",
    "    if pos_tags_to_remove:\n",
    "        text = remove_tags(text, pos_tags_to_remove)\n",
    "    \n",
    "    return text.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dataset['cleaned_facts_pos_removed'] = df_dataset['cleaned_facts'].apply(lambda x: remove_pos(x, True, True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dataset['cleaned_bertsum_pos_removed'] = df_dataset['cleaned_bertsum'].apply(lambda x: remove_pos(x, True, True))\n",
    "df_dataset['cleaned_textrank_pos_removed'] = df_dataset['cleaned_textrank'].apply(lambda x: remove_pos(x, True, True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dataset.to_csv('df_dataset.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dataset['cleaned_facts'].to_csv('df_dataset_cleaned_facts.csv')\n",
    "df_dataset['cleaned_bertsum'].to_csv('df_dataset_cleaned_bertsum.csv')\n",
    "df_dataset['cleaned_facts_pos_removed'].to_csv('df_dataset_cleaned_facts_pos_removed.csv')\n",
    "df_dataset['cleaned_bertsum_pos_removed'].to_csv('df_dataset_cleaned_bertsum_pos_removed.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "echr_foe",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
